---
title: "Ziqiu Practice"
author: "Ziqiu"
date: "June 7, 2016"
output: html_document
---

This practice focuses on Sentiment Analysis in R. You will provide a written analysis based on the following information:

Load the file ML.Tweets.csv and ML.Tweets.New.csv (it is online at ‘http://nikbearbrown.com/YouTube/MachineLearning/M08/ML.Tweets.csv’ and ‘http://nikbearbrown.com/YouTube/MachineLearning/M08/ML.Tweets.New.csv’ )

Complete following with ML.Tweets.csv:
Extract and rank a list of the important hashtags (using td-idf or word entropy).
Cluster the tweets using these hashtags.
Optional - Give the the clusters names based on their dominant hashtags.
Classify the tweets in ML.Tweets.New.csv using the cluster labels generated from ML.Tweets.csv.
Use the qdap polarity function to score the polarity of the tweets in ML.Tweets.csv.
Would creating a custom polarity.frame - A dataframe or environment containing a dataframe of positive/negative words and weights - based on the tags and words in these tweets improve the polarity score? Try it.
```{r}
tw<-read.csv("http://nikbearbrown.com/YouTube/MachineLearning/M08/M08_tweets.csv",header = FALSE)
tw1<-as.matrix(tw)
# remove retweet entities
tw1 <- gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", tw)
# remove at people
tw1 <- gsub("@\\w+", "", tw1)
# remove punctuation
tw1 <- gsub("(?!#)[[:punct:]]", "", tw1,perl = TRUE)
# remove numbers
tw1 <- gsub("[[:digit:]]", "", tw1)
# remove html links
tw1 <- gsub("http\\w+", "", tw1)
# remove unnecessary spaces
tw1 <- gsub("[ \t]{2,}", "", tw1)
tw1<- gsub("^\\s+|\\s+$", "", tw1)
#gregexpr("#\\w+",tw,perl = TRUE)
ht<-regmatches(tw1,gregexpr("#\\w+",tw1,perl = TRUE))
ht<-ht[lapply(ht,length)>0]
ht1<-unlist(ht)
ht1<-gsub("#","",ht1)
library(tm)
ht1.corpus <- Corpus(DataframeSource(data.frame(ht1)))
ht1.clean<-tm_map(ht1.corpus, stripWhitespace)
ht1.clean.lc <- tm_map(ht1.clean, content_transformer(tolower))
ht1.clean <- tm_map(ht1.clean.lc, removeWords, stopwords("english"))
ht1.tdm <- TermDocumentMatrix(ht1.clean, control = list(minWordLength = 1))
ht1.tdm
findFreqTerms(ht1.tdm, lowfreq=500)
#so the most frequency terms are here
#[1] "unityd"       "kca"          "ipadgames"    "ipad"         "indiegame"    "indiedev"    
#[7] "gameinsight"  "gamedev"      "game"         "bigdata"      "androidgames" "android"     
str1.corpus <- Corpus(DataframeSource(data.frame(tw)))
#inspect(str1.corpus)
str1.clean<-tm_map(str1.corpus, stripWhitespace)
str1.clean.lc <- tm_map(str1.clean, content_transformer(tolower))
str1.clean <- tm_map(str1.clean.lc, removeWords, stopwords("english"))
str1.tdm <- TermDocumentMatrix(str1.clean, control = list(minWordLength = 1))
str1.tdm1 <- removeSparseTerms(str1.tdm, sparse=0.95)
m1<-as.matrix(str1.tdm1)
distMatrix <- dist(scale(m1))
fit <- hclust(distMatrix, method="ward.D")

plot(fit)
# cut tree into 10 clusters
m2<- t(m1)
set.seed(122)
k <- 10
kmeansResult <- kmeans(m2, k)
for (i in 1:k) {
  cat(paste("cluster ", i, ":  ", sep=""))
  s <- sort(kmeansResult$centers[i,], decreasing=T)
  cat(names(s)[1:3], "\n")
}
```


